{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Networks Using Blocks (VGG)\n",
    ":label:`sec_vgg`\n",
    "\n",
    "While AlexNet offered empirical evidence that deep CNNs\n",
    "can achieve good results, it did not provide a general template\n",
    "to guide subsequent researchers in designing new networks.\n",
    "In the following sections, we will introduce several heuristic concepts\n",
    "commonly used to design deep networks.\n",
    "\n",
    "Progress in this field mirrors that in chip design\n",
    "where engineers went from placing transistors\n",
    "to logical elements to logic blocks.\n",
    "Similarly, the design of neural network architectures\n",
    "had grown progressively more abstract,\n",
    "with researchers moving from thinking in terms of\n",
    "individual neurons to whole layers,\n",
    "and now to blocks, repeating patterns of layers.\n",
    "\n",
    "The idea of using blocks first emerged from the\n",
    "[Visual Geometry Group](http://www.robots.ox.ac.uk/~vgg/) (VGG)\n",
    "at Oxford University,\n",
    "in their eponymously-named *VGG* network.\n",
    "It is easy to implement these repeated structures in code\n",
    "with any modern deep learning framework by using loops and subroutines.\n",
    "\n",
    "\n",
    "## (**VGG Blocks**)\n",
    ":label:`subsec_vgg-blocks`\n",
    "\n",
    "The basic building block of classic CNNs\n",
    "is a sequence of the following:\n",
    "(i) a convolutional layer\n",
    "with padding to maintain the resolution,\n",
    "(ii) a nonlinearity such as a ReLU,\n",
    "(iii) a pooling layer such\n",
    "as a maximum pooling layer.\n",
    "One VGG block consists of a sequence of convolutional layers,\n",
    "followed by a maximum pooling layer for spatial downsampling.\n",
    "In the original VGG paper :cite:`Simonyan.Zisserman.2014`,\n",
    "the authors\n",
    "employed convolutions with $3\\times3$ kernels with padding of 1 (keeping height and width)\n",
    "and $2 \\times 2$ maximum pooling with stride of 2\n",
    "(halving the resolution after each block).\n",
    "In the code below, we define a function called `vgg_block`\n",
    "to implement one VGG block.\n"
   ],
   "metadata": {
    "origin_pos": 0
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The function takes three arguments corresponding to the number\n",
    "of convolutional layers `num_convs`, the number of input channels `in_channels`\n",
    "and the number of output channels `out_channels`.\n"
   ],
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "def vgg_block(num_convs, in_channels, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "    return nn.Sequential(*layers)"
   ],
   "outputs": [],
   "metadata": {
    "origin_pos": 4,
    "tab": [
     "pytorch"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## [**VGG Network**]\n",
    "\n",
    "Like AlexNet and LeNet,\n",
    "the VGG Network can be partitioned into two parts:\n",
    "the first consisting mostly of convolutional and pooling layers\n",
    "and the second consisting of fully-connected layers.\n",
    "This is depicted in :numref:`fig_vgg`.\n",
    "\n",
    "![From AlexNet to VGG that is designed from building blocks.](../img/vgg.svg)\n",
    ":width:`400px`\n",
    ":label:`fig_vgg`\n",
    "\n",
    "\n",
    "The convolutional part of the network connects several VGG blocks from :numref:`fig_vgg` (also defined in the `vgg_block` function)\n",
    "in succession.\n",
    "The following variable `conv_arch` consists of a list of tuples (one per block),\n",
    "where each contains two values: the number of convolutional layers\n",
    "and the number of output channels,\n",
    "which are precisely the arguments required to call\n",
    "the `vgg_block` function.\n",
    "The fully-connected part of the VGG network is identical to that covered in AlexNet.\n",
    "\n",
    "The original VGG network had 5 convolutional blocks,\n",
    "among which the first two have one convolutional layer each\n",
    "and the latter three contain two convolutional layers each.\n",
    "The first block has 64 output channels\n",
    "and each subsequent block doubles the number of output channels,\n",
    "until that number reaches 512.\n",
    "Since this network uses 8 convolutional layers\n",
    "and 3 fully-connected layers, it is often called VGG-11.\n"
   ],
   "metadata": {
    "origin_pos": 6
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))"
   ],
   "outputs": [],
   "metadata": {
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following code implements VGG-11. This is a simple matter of executing a for-loop over `conv_arch`.\n"
   ],
   "metadata": {
    "origin_pos": 8
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def vgg(conv_arch):\n",
    "    conv_blks = []\n",
    "    in_channels = 1\n",
    "    # The convolutional part\n",
    "    for (num_convs, out_channels) in conv_arch:\n",
    "        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))\n",
    "        in_channels = out_channels\n",
    "\n",
    "    return nn.Sequential(\n",
    "        *conv_blks, nn.Flatten(),\n",
    "        # The fully-connected part\n",
    "        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 10))\n",
    "\n",
    "net = vgg(conv_arch)"
   ],
   "outputs": [],
   "metadata": {
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we will construct a single-channel data example\n",
    "with a height and width of 224 to [**observe the output shape of each layer**].\n"
   ],
   "metadata": {
    "origin_pos": 12
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "X = torch.randn(size=(1, 1, 224, 224))\n",
    "for blk in net:\n",
    "    X = blk(X)\n",
    "    print(blk.__class__.__name__, 'output shape:\\t', X.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 112, 112])\n",
      "Sequential output shape:\t torch.Size([1, 128, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 256, 28, 28])\n",
      "Sequential output shape:\t torch.Size([1, 512, 14, 14])\n",
      "Sequential output shape:\t torch.Size([1, 512, 7, 7])\n",
      "Flatten output shape:\t torch.Size([1, 25088])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "metadata": {
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, we halve height and width at each block,\n",
    "finally reaching a height and width of 7\n",
    "before flattening the representations\n",
    "for processing by the fully-connected part of the network.\n",
    "\n",
    "## Training\n",
    "\n",
    "[**Since VGG-11 is more computationally-heavy than AlexNet\n",
    "we construct a network with a smaller number of channels.**]\n",
    "This is more than sufficient for training on Fashion-MNIST.\n"
   ],
   "metadata": {
    "origin_pos": 16
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "ratio = 4\n",
    "small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch]\n",
    "net = vgg(small_conv_arch)"
   ],
   "outputs": [],
   "metadata": {
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apart from using a slightly larger learning rate,\n",
    "the [**model training**] process is similar to that of AlexNet in :numref:`sec_alexnet`.\n"
   ],
   "metadata": {
    "origin_pos": 19
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lr, num_epochs, batch_size = 0.05, 10, 128\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\n",
    "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
   ],
   "outputs": [],
   "metadata": {
    "origin_pos": 20,
    "tab": [
     "pytorch"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "\n",
    "* VGG-11 constructs a network using reusable convolutional blocks. Different VGG models can be defined by the differences in the number of convolutional layers and output channels in each block.\n",
    "* The use of blocks leads to very compact representations of the network definition. It allows for efficient design of complex networks.\n",
    "* In their VGG paper, Simonyan and Ziserman experimented with various architectures. In particular, they found that several layers of deep and narrow convolutions (i.e., $3 \\times 3$) were more effective than fewer layers of wider convolutions.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. When printing out the dimensions of the layers we only saw 8 results rather than 11. Where did the remaining 3 layer information go?\n",
    "1. Compared with AlexNet, VGG is much slower in terms of computation, and it also needs more GPU memory. Analyze the reasons for this.\n",
    "1. Try changing the height and width of the images in Fashion-MNIST from 224 to 96. What influence does this have on the experiments?\n",
    "1. Refer to Table 1 in the VGG paper :cite:`Simonyan.Zisserman.2014` to construct other common models, such as VGG-16 or VGG-19.\n"
   ],
   "metadata": {
    "origin_pos": 21
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/78)\n"
   ],
   "metadata": {
    "origin_pos": 23,
    "tab": [
     "pytorch"
    ]
   }
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('d2l': conda)"
  },
  "interpreter": {
   "hash": "055c7ff15205f2dd48c72f407e5e7463f827bd67688902f3078972f721503001"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}